"""
Snake Energy Loss Terms in CCP Framework:

1. Elastic Term (Continuity): TVLoss
   - TV (Total Variation) loss: ì—°ì†ì„±ì„ ê°•ì œí•˜ì—¬ contourê°€ ë¶€ë“œëŸ½ê²Œ ì—°ê²°ë˜ë„ë¡
   - Snake energyì˜ Î± * |ds/dx|Â² termì— ëŒ€ì‘
   - weight key: 'tv', 'tv_init', 'tv_coarse', 'tv_evolve', 'tv_evolve_{i}'

2. Bending Term (Curvature): CurvLoss
   - Snake energyì˜ Î² * |dÂ²s/dxÂ²|Â² termì— ëŒ€ì‘
   - weight key: 'cv', 'cv_init', 'cv_coarse', 'cv_evolve', 'cv_evolve_{i}'
   - loss_type 'l2' ì„¤ì •ì‹œ ì •í™•í•œ snake energyì™€ ì¼ì¹˜

3. External Energy: pixel loss, ct loss ë“±
   - ì´ë¯¸ì§€ featureì— ê¸°ë°˜í•œ external force
   - Snake energyì˜ external energy termì— ëŒ€ì‘

Configuration ì˜ˆì‹œ:
weight_dict = {
    'tv': 1.0,      # elastic term weight
    'cv': 0.1,      # bending term weight  
}

loss_type = {
    'tv': 'l2',     # elastic loss type
    'cv': 'l2',     # bending loss type (ì •í™•í•œ snake energy)
}
"""

import torch.nn as nn
from .utils import (FocalLoss, DMLoss, sigmoid, TVLoss, CurvLoss, MDLoss, mIoULoss, EdgeStandardDeviationLoss, BoundedRegLoss,
                    SoftCELoss, FocalCELoss, CosineSimLoss, SoftBCELoss, MeanSimLoss, CDLoss, VertexClsLoss, FocalBCELoss, TemperatureFocalCELoss)
import torch
import torch.nn.functional as F
import cv2, random
import numpy as np
import torch.distributed as dist

def check_nan(name, tensor):
    if torch.isnan(tensor).any() or torch.isinf(tensor).any():
        print(f"[NaN detected] {name} has NaN or Inf!")

def safe_empty_tensor(tensor):
    if tensor.numel() == 0:
        # tensor shape ì˜ˆ: (0, 64, 2) => (1, 64, 2) ì´ë ‡ê²Œ ìˆ˜ì •
        shape = list(tensor.shape)
        if shape[0] == 0:
            shape[0] = 1  # batch dimë§Œ 1ë¡œ ë³€ê²½
        empty_tensor = torch.zeros(
            shape,
            dtype=tensor.dtype,
            device=tensor.device,
            requires_grad=tensor.requires_grad
        )
        return empty_tensor
    else:
        return tensor

def safe_output(output):
    def _safe(v):
        if isinstance(v, torch.Tensor):
            return safe_empty_tensor(v)
        elif isinstance(v, list):
            return [_safe(t) for t in v]
        elif isinstance(v, dict):
            return {kk: _safe(vv) for kk, vv in v.items()}
        else:
            return v

    return {k: _safe(v) for k, v in output.items()}


class NetworkWrapper(nn.Module):
    def __init__(self, net, with_dml=True, ml_start_epoch=10, weight_dict=None, cfg=None):
        super(NetworkWrapper, self).__init__()
        self.cfg = cfg
        self.with_dml = with_dml
        self.ml_start_epoch = ml_start_epoch
        self.dml_start_epoch = self.cfg.train.dml_start_epoch
        self.mdml_start_epoch = self.cfg.train.mdml_start_epoch
        self.net = net
        self.weight_dict = weight_dict
        self.loss_dict = nn.ModuleDict()

        if cfg.train.weight_dict.get("vertex_cls", 0.) > 0:
            self.loss_dict["vertex_cls"] = VertexClsLoss(**self.cfg.train.loss_params["vertex_cls"])

        if 'pixel' in self.cfg.model.heads:
            if self.cfg.model.heads['pixel'] == 1:
                self.pix_crit = FocalLoss()
                self.pix_type = 'focal_single'
            else:
                self.pix_type = self.cfg.train.loss_type['pixel'] if 'pixel' in self.cfg.train.loss_type else 'ce'
                if self.pix_type == 'focal':
                    self.pix_crit = FocalCELoss(gamma=self.cfg.train.loss_params['pixel']['gamma'] if 'gamma' in self.cfg.train.loss_params['pixel'] else 2,
                                                reduce=self.cfg.train.loss_params['pixel']['reduce'] if 'reduce' in self.cfg.train.loss_params['pixel'] else True)
                elif self.pix_type == 'focal_bce':
                    gamma = self.cfg.train.loss_params['pixel']['gamma'] if 'gamma' in self.cfg.train.loss_params['pixel'] else 2.0
                    alpha_fg = self.cfg.train.loss_params['pixel']['alpha_fg'] if 'alpha_fg' in self.cfg.train.loss_params['pixel'] else 0.5
                    alpha_bg = self.cfg.train.loss_params['pixel']['alpha_bg'] if 'alpha_bg' in self.cfg.train.loss_params['pixel'] else 0.25
                    reduction = "mean" if self.cfg.train.loss_params['pixel'].get('reduce', True) else "none"
                    self.pix_crit = FocalBCELoss(gamma=gamma, alpha_fg=alpha_fg, alpha_bg=alpha_bg, reduction=reduction)
                elif self.pix_type == 'temperature_focal':
                    # trainable_softmax íƒ€ì…ì¼ ë•Œ ì‚¬ìš©ë˜ëŠ” temperature focal CE loss
                    self.pix_crit = TemperatureFocalCELoss(gamma=self.cfg.train.loss_params['pixel']['gamma'] if 'gamma' in self.cfg.train.loss_params['pixel'] else 2,
                                                          reduce=self.cfg.train.loss_params['pixel']['reduce'] if 'reduce' in self.cfg.train.loss_params['pixel'] else True)
                else:
                    self.pix_crit = torch.nn.functional.cross_entropy

        self.ct_crit = FocalLoss()
        self.py_crit = torch.nn.functional.smooth_l1_loss
        self.tv_crit = TVLoss(type=cfg.train.loss_type['tv'] if cfg is not None else 'smooth_l1')
        self.cv_crit = CurvLoss(type=cfg.train.loss_type['cv'] if cfg is not None else 'smooth_l1')
        if self.cfg.model.with_rasterize_net:
            self.region_crit = mIoULoss(n_classes=2)

        if with_dml:
            self.ml_crit = DMLoss(type=cfg.train.loss_type['dm'] if cfg is not None else 'smooth_l1')
        elif cfg.train.with_mdl:
            self.ml_crit = MDLoss(type=cfg.train.loss_type['md'] if cfg is not None else 'smooth_l1',
                                  match_with_ini=cfg.train.ml_match_with_ini if cfg is not None else True)
        else:
            self.ml_crit = self.py_crit

        PY_RANGE_DICT = {'none': [],
                         'final': [self.cfg.model.evolve_iters - 1],
                         'penultimate': [self.cfg.model.evolve_iters - 2],
                         'last2': [i for i in range(self.cfg.model.evolve_iters - 2, self.cfg.model.evolve_iters)],
                         'except_1st': [i for i in range(1, self.cfg.model.evolve_iters)],
                         'all': [i for i in range(self.cfg.model.evolve_iters)]}
        self.ml_range_py = PY_RANGE_DICT[self.cfg.train.ml_range_py]
        if self.cfg.train.dml_range != 'none':
            self.dml_range = PY_RANGE_DICT[self.cfg.train.dml_range]
            self.dml_crit = DMLoss(type=cfg.train.loss_type['dm'] if cfg is not None else 'smooth_l1')
        else:
            self.dml_range = []
        if self.cfg.train.mdml_range != 'none':
            self.mdml_range = PY_RANGE_DICT[self.cfg.train.mdml_range]
            self.mdml_crit = MDLoss(type=cfg.train.loss_type['md'] if cfg is not None else 'smooth_l1',
                                  match_with_ini=cfg.train.ml_match_with_ini if cfg is not None else True)
        else:
            self.mdml_range = []
        if ('edge_std' in self.cfg.train.weight_dict) or ('edge_std_init' in self.cfg.train.weight_dict) or ('edge_std_coarse' in self.cfg.train.weight_dict) or ('edge_std_evolve' in self.cfg.train.weight_dict):
            self.eeq_crit = EdgeStandardDeviationLoss()
        else:
            self.eeq_crit = None
        if self.cfg.model.with_sharp_contour:
            self.ipc_crit = nn.BCEWithLogitsLoss()
        if 'kd' in cfg.commen.task:
            self.kd_cls_crit = SoftCELoss(T=self.cfg.train.kd_param['soft_T'] if 'soft_T' in self.cfg.train.kd_param else 10.)
            self.kd_bcls_crit = SoftBCELoss(
                T=self.cfg.train.kd_param['soft_T'] if 'soft_T' in self.cfg.train.kd_param else 10.)
            self.kd_reg_crit = BoundedRegLoss(type=cfg.train.loss_type['kd_reg'] if 'kd_reg' in cfg.train.loss_type else 'smooth_l1',
                                              condition_type=cfg.train.kd_param['reg_condition_type'] if 'reg_condition_type' in cfg.train.kd_param else 'error',
                                              margin=cfg.train.kd_param['reg_margin'] if 'reg_margin' in cfg.train.kd_param else 0)
            inter_type = cfg.train.kd_param['inter_type'] if 'inter_type' in cfg.train.kd_param else 'cosine'
            if 'mean' in inter_type:
                self.kd_inter_crit = MeanSimLoss(sim_type=inter_type.split('_')[-1])
            elif 'cd' in inter_type:
                self.kd_inter_crit = CDLoss(soft_param=self.cfg.train.kd_param['feature_soft_param'] if 'feature_soft_param' in self.cfg.train.kd_param else 1)
            else:
                self.kd_inter_crit = CosineSimLoss(apply_type=self.cfg.train.kd_param[
                    'feature_apply_type'] if 'feature_apply_type' in self.cfg.train.kd_param else 'channel')

        # DDP-safe anchor param (ê°€ë²¼ì›€; ëª¨ë“  stepì—ì„œ ê·¸ë˜í”„ ì—°ê²° ë³´ì¥), edit:debug:ddp-stop:25-08-09
        self.register_buffer("_ddp_anchor", torch.zeros((), dtype=torch.float32), persistent=False)

    def forward(self, batch, mode='default', output_t=None):
        # print(f"[NETWORK WRAPPER] meta={batch.get('meta')}")
        if 'test' in batch['meta']:
            output = self.net(batch['inp'], batch=batch)
            return output
        else:
            output = self.net(batch['inp'], batch=batch)
            return self.compute_loss(output, batch, output_t=output_t, mode=mode)

    def compute_loss(self, output, batch, output_t=None, mode='default'):
        out_ontraining = {}
        epoch = batch['epoch']
        scalar_stats = {}
        # í•­ìƒ í…ì„œ+ê·¸ë˜í”„ë¡œ ì‹œì‘ (rank/step ë™ì¼í•˜ê²Œ ì¡´ì¬), DDP-safe, edit:debug:ddp-stop:25-08-09
        loss = self._ddp_anchor * 0.0
        dummy = self._ddp_anchor * 0.0

        keyPointsMask = batch['keypoints_mask'][batch['ct_01']]
        # vertex cls
        if self.cfg.train.weight_dict.get("vertex_cls", 0.) > 0:
            # ğŸš¨ DDP-safe check: 'py_valid_logits'ê°€ ë¹„ì–´ìˆëŠ” ê²½ìš°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
            if 'py_valid_logits' in output and len(output['py_valid_logits']) > 0:
                vertex_cls_loss = dummy
                for py_valid_logit in output['py_valid_logits']:
                    pred_vertex_logits = py_valid_logit.permute(0, 2, 1)
                    # print(pred_vertex_logits.shape)
                    pred_coords = output['py_pred'][-1]
                    vertex_gt_coord = output['img_gt_polys']
                    vertex_cls_loss += self.loss_dict["vertex_cls"](pred_vertex_logits, pred_coords, vertex_gt_coord, keyPointsMask)/len(output['py_valid_logits'])
            else:
                # 'py_valid_logits'ê°€ ì—†ê±°ë‚˜ ë¹„ì–´ìˆìœ¼ë©´ dummy_lossë¥¼ ì‚¬ìš©í•˜ì—¬ ì—°ì‚° ê·¸ë˜í”„ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.
                vertex_cls_loss = dummy
            scalar_stats.update({'vtx_cls_loss': vertex_cls_loss})
            weight_vertex_cls = self.cfg.train.weight_dict.get("vertex_cls", 0.)
            loss += weight_vertex_cls * vertex_cls_loss

        # pixel
        if 'pixel' in self.cfg.model.heads:
            if isinstance(output['pixel'], list):
                to_size = (output['pixel'][-1].size(-2), output['pixel'][-1].size(-1))
            else:
                to_size = (output['pixel'].size(-2), output['pixel'].size(-1))

            pixel_gt = F.interpolate(batch['pixel_gt'].unsqueeze(1).float(), size=to_size, mode='nearest').squeeze(1)
            for pixelmap_i in range(len(output['pixel'])):
                if self.cfg.model.heads['pixel'] == 1:
                    pix_loss = self.pix_crit(sigmoid(output['pixel'][pixelmap_i]), pixel_gt.bool().float())
                elif self.pix_type == 'focal_bce':
                    # focal_bceëŠ” 2 ì±„ë„ë¡œ ì¶œë ¥í•˜ê³  binary target ì‚¬ìš©
                    pix_loss = self.pix_crit(output['pixel'][pixelmap_i], pixel_gt.bool().long())
                elif self.pix_type == 'temperature_focal' and self.cfg.model.ccp_deform_pixel_norm in ['trainable_softmax', 'trainable_softmax_softclamp']:
                    # trainable_softmax ë˜ëŠ” trainable_softmax_softclamp íƒ€ì…ì¼ ë•Œ temperature íŒŒë¼ë¯¸í„° ì‚¬ìš©
                    # Evolution ëª¨ë“ˆì—ì„œ temperature ê°€ì ¸ì˜¤ê¸° - gcnì´ Evolution ëª¨ë“ˆì´ë¯€ë¡œ ì§ì ‘ ì ‘ê·¼
                    temperature = None
                    
                    # trainable_softmax: self.net.gcn.temperature ì‚¬ìš©
                    if hasattr(self.net, 'gcn') and hasattr(self.net.gcn, 'temperature'):
                        temperature = self.net.gcn.temperature
                    # trainable_softmax_softclamp: u íŒŒë¼ë¯¸í„°ì—ì„œ temperature ê³„ì‚°
                    elif hasattr(self.net, 'gcn') and hasattr(self.net.gcn, 'u') and hasattr(self.net.gcn, 'T_lo') and hasattr(self.net.gcn, 'T_hi'):
                        # T = T_lo + (T_hi - T_lo) * sigmoid(u)
                        u = self.net.gcn.u
                        T_lo = self.net.gcn.T_lo
                        T_hi = self.net.gcn.T_hi
                        temperature = T_lo + (T_hi - T_lo) * torch.sigmoid(u)
                    
                    if temperature is not None:
                        pix_loss = self.pix_crit(output['pixel'][pixelmap_i], pixel_gt.bool().long(), temperature)
                    else:
                        # fallback to regular focal CE loss (use FocalCELoss instead of raw cross_entropy)
                        fallback_focal = FocalCELoss(gamma=2, reduce=True)
                        pix_loss = fallback_focal(output['pixel'][pixelmap_i], pixel_gt.bool().long())
                else:
                    if hasattr(self, 'pix_crit') and callable(self.pix_crit):
                        pix_loss = self.pix_crit(output['pixel'][pixelmap_i], pixel_gt.bool().long())
                    else:
                        # deterministic safe fallback using FocalCELoss
                        fallback_focal = FocalCELoss(gamma=2, reduce=True)
                        pix_loss = fallback_focal(output['pixel'][pixelmap_i], pixel_gt.bool().long())
                scalar_stats.update({f'pix_loss{pixelmap_i}': pix_loss})
                if f'pixel_{pixelmap_i}' in self.weight_dict:
                    weight_pix = self.weight_dict[f'pixel_{pixelmap_i}']
                elif 'pixel' in self.weight_dict:
                    weight_pix = self.weight_dict['pixel']
                else:
                    weight_pix = 1.
                if self.cfg.train.is_normalize_pixel:
                    loss += weight_pix * pix_loss/len(output['pixel'])
                else:
                    loss += weight_pix * pix_loss

        # ct
        if (output_t is not None) and ('ct' in self.cfg.train.kd_param['losses']) and (
                self.cfg.train.kd_param['weight_type'] == 'normalized'):
            if f'kd_ct' in self.weight_dict:
                weight_kd = self.weight_dict[f'kd_ct']
            elif 'kd' in self.weight_dict:
                weight_kd = self.weight_dict['kd']
            else:
                weight_kd = 0.5
            if weight_kd >= 1.:
                weight_kd = weight_kd / (1 + weight_kd)
            weight_ct = self.weight_dict['box_ct'] * (1 - weight_kd)
        else:
            weight_ct = self.weight_dict['box_ct']

        if self.weight_dict['box_ct'] > 0:
            # ğŸš¨ DDP-unsafeí•œ ì¡°ê±´ë¬¸ ìˆ˜ì •: ct_hmì´ 0ì¸ ê²½ìš°ì—ë„ dummy_lossë¥¼ ì‚¬ìš©í•˜ì—¬ ì—°ì‚° ê·¸ë˜í”„ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.
            if batch['ct_hm'].sum() == 0 and epoch < 5:
                ct_loss = dummy
            else:
                ct_loss = self.ct_crit(sigmoid(output['ct_hm']), batch['ct_hm'])
            # print(f"({dist.get_rank()})[DEBUG] ct_loss requires_grad:", ct_loss.requires_grad)
            scalar_stats.update({'ct_loss': ct_loss})
            loss += weight_ct * ct_loss

        # init & coarse
        if self.cfg.model.with_img_idx:
            poly_init = output['poly_init'][batch['ct_01']]
            poly_coarse = output['poly_coarse'][batch['ct_01']]
            py_pred = []
            # ccp taskì—ì„œëŠ” py_pred ëŒ€ì‹  pyë¥¼ ì‚¬ìš©
            if 'py_pred' in output:
                for py in output['py_pred']:
                    py_pred.append(py[batch['ct_01']])
            elif 'py' in output:
                for py in output['py']:
                    py_pred.append(py[batch['ct_01']])
            # Stage 1ì—ì„œ evolve_iters=0ì´ë©´ py_pred/pyê°€ ì—†ì„ ìˆ˜ ìˆìŒ (ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ìœ ì§€)
        else:
            poly_init = output['poly_init']
            poly_coarse = output['poly_coarse']
            # ccp taskì—ì„œëŠ” py_pred ëŒ€ì‹  pyë¥¼ ì‚¬ìš©
            if 'py_pred' in output:
                py_pred = output['py_pred']
            elif 'py' in output:
                py_pred = output['py']
            else:
                # Stage 1ì—ì„œ evolve_iters=0ì´ë©´ py_predê°€ ì—†ì„ ìˆ˜ ìˆìŒ
                py_pred = []

        num_polys = len(poly_init)

        if num_polys == 0:
            init_py_loss = dummy
            coarse_py_loss = dummy
        else:
            # GT polygons: ccp_maskinitëŠ” ë³„ë„ í‚¤ ì‚¬ìš©, ccpëŠ” img_gt_polys ê³µìš©
            gt_init_polys = output.get('img_gt_init_polys', output.get('img_gt_polys', []))
            gt_coarse_polys = output.get('img_gt_coarse_polys', output.get('img_gt_polys', []))
            
            # GTê°€ ì—†ìœ¼ë©´ dummy loss ì‚¬ìš©
            if len(gt_init_polys) == 0 or len(gt_coarse_polys) == 0:
                init_py_loss = dummy
                coarse_py_loss = dummy
            else:
                # print(f"poly_init] :  {poly_init.max()}, gt_init_polys : {gt_init_polys.max()}")
                init_py_loss = self.py_crit(poly_init, gt_init_polys)
                coarse_py_loss = self.py_crit(poly_coarse, gt_coarse_polys)

        # print(f"({dist.get_rank()})[DEBUG] init_py_loss requires_grad:", init_py_loss.requires_grad)
        # print(f"({dist.get_rank()})[DEBUG] coarse_py_loss requires_grad:", coarse_py_loss.requires_grad)
        if (output_t is not None) and ('init' in self.cfg.train.kd_param['losses']) and (
                self.cfg.train.kd_param['weight_type'] == 'normalized'):
            if f'kd_init' in self.weight_dict:
                weight_kd = self.weight_dict[f'kd_init']
            elif 'kd' in self.weight_dict:
                weight_kd = self.weight_dict['kd']
            else:
                weight_kd = 0.5
            if weight_kd >= 1.:
                weight_kd = weight_kd / (1 + weight_kd)
            weight_py = self.weight_dict['init'] * (1 - weight_kd)
            print(f"weight_py (with kd) : {weight_py}")
        else:
            weight_py = self.weight_dict['init']
        
        
        if self.weight_dict['init'] > 0 and weight_py > 0:
            scalar_stats.update({'init_py_loss': init_py_loss})
            loss += init_py_loss * weight_py

        if (output_t is not None) and ('coarse' in self.cfg.train.kd_param['losses']) and (
                self.cfg.train.kd_param['weight_type'] == 'normalized'):
            if f'kd_coarse' in self.weight_dict:
                weight_kd = self.weight_dict[f'kd_coarse']
            elif 'kd' in self.weight_dict:
                weight_kd = self.weight_dict['kd']
            else:
                weight_kd = 0.5
            if weight_kd >= 1.:
                weight_kd = weight_kd / (1 + weight_kd)
            weight_py = self.weight_dict['coarse'] * (1 - weight_kd)
            print(f"weight_py (with kd) : {weight_py}")
        else:
            weight_py = self.weight_dict['coarse']
        if self.weight_dict['coarse'] > 0:
            scalar_stats.update({'coarse_py_loss': coarse_py_loss})
            loss += coarse_py_loss * weight_py

        if self.ml_range_py:
            special_loss_range = self.ml_range_py
            special_loss_start_epoch = self.ml_start_epoch
        elif self.dml_range:
            special_loss_range = self.dml_range
            special_loss_start_epoch = self.dml_start_epoch
        elif self.mdml_range:
            special_loss_range = self.mdml_range
            special_loss_start_epoch = self.mdml_start_epoch
        else:
            special_loss_range = []
            special_loss_start_epoch = 0

        # evolve lossëŠ” py_predê°€ ìˆì„ ë•Œë§Œ ê³„ì‚° (Stage 1ì—ì„œëŠ” evolve_iters=0ì´ë¯€ë¡œ py_predê°€ ì—†ì„ ìˆ˜ ìˆìŒ)
        if self.weight_dict['evolve'] > 0 and len(py_pred) > 0:
            # py_loss = dummy
            n = len(py_pred)
            for i in range(n):
                if (output_t is not None) and (f'evolve_{i}' in self.cfg.train.kd_param['losses']) and (
                        self.cfg.train.kd_param['weight_type'] == 'normalized'):
                    if f'kd_py_{i}' in self.weight_dict:
                        weight_kd = self.weight_dict[f'kd_py_{i}']
                    elif 'kd' in self.weight_dict:
                        weight_kd = self.weight_dict['kd']
                    else:
                        weight_kd = 0.5
                    if weight_kd >= 1.:
                        weight_kd = weight_kd / (1 + weight_kd)
                    weight_py = self.weight_dict['evolve'] * (1 - weight_kd)
                    print(f"weight_py (with kd) : {weight_py}")
                else:
                    weight_py = self.weight_dict['evolve']
                # print(f"py_pred[i] :  {py_pred[i].max()}, output['img_gt_polys'] : {output['img_gt_polys'].max()}")
                if num_polys == 0:
                    part_loss = dummy
                else:
                    # ğŸš¨ DDP-safe check: Ground Truthê°€ ì—†ëŠ” ê²½ìš°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
                    if i in special_loss_range and epoch >= special_loss_start_epoch:
                        if len(output['img_gt_polys']) == 0:
                            part_loss = dummy
                        else:
                            if self.with_dml:
                                part_loss = self.ml_crit(py_pred[i - 1], py_pred[i], output['img_gt_polys'],
                                                         keyPointsMask)
                            elif self.cfg.train.with_mdl:
                                part_loss = self.ml_crit(py_pred[i - 1], py_pred[i], output['img_gt_polys']).mean()
                            else:
                                part_loss = self.py_crit(py_pred[i], output['img_gt_polys'])
                    else:
                        part_loss = self.py_crit(py_pred[i], output['img_gt_polys'])

                scalar_stats.update({f'py_loss_{i}': part_loss})
                loss += part_loss / len(py_pred) * weight_py

        ## total variation
        tv_weight_keys = [k for k in self.weight_dict.keys() if k.startswith('tv')]
        if tv_weight_keys:
            if num_polys == 0:
                init_tv_loss = dummy
                coarse_tv_loss = dummy
            else:
                init_tv_loss = self.tv_crit(poly_init)
                coarse_tv_loss = self.tv_crit(poly_coarse)
            if 'tv_init' in self.weight_dict:
                weight_tv_init = self.weight_dict['tv_init']
            else:
                weight_tv_init = self.weight_dict.get('init', 0) * self.weight_dict.get('tv', 0)
            if 'tv_coarse' in self.weight_dict:
                weight_tv_coarse = self.weight_dict['tv_coarse']
            else:
                weight_tv_coarse = self.weight_dict['coarse'] * self.weight_dict['tv']
            # tv_evolve weight ì²˜ë¦¬: ê° iterationë³„ë¡œ ë‹¤ë¥¸ weight ì ìš© ê°€ëŠ¥
            # tv_evolve_0, tv_evolve_1, ... í˜•íƒœë¡œ ê°œë³„ weight ì„¤ì • ê°€ëŠ¥
            # ì—†ìœ¼ë©´ tv_evolve ì‚¬ìš©, ê·¸ê²ƒë„ ì—†ìœ¼ë©´ evolve * tv ì‚¬ìš©
            if weight_tv_init > 0:
                scalar_stats.update({'init_tv_loss': init_tv_loss})
                loss += init_tv_loss * weight_tv_init
            if weight_tv_coarse > 0:
                scalar_stats.update({'coarse_tv_loss': coarse_tv_loss})
                loss += coarse_tv_loss * weight_tv_coarse
            
            # evolve TV loss - iterationë³„ ê°œë³„ weight ì§€ì›
            if len(py_pred) > 0 and 'tv' in self.weight_dict:
                for i in range(len(py_pred)):
                    # iterationë³„ ê°œë³„ weight í™•ì¸
                    if f'tv_evolve_{i}' in self.weight_dict:
                        weight_tv_evolve_i = self.weight_dict[f'tv_evolve_{i}']
                    elif 'tv_evolve' in self.weight_dict:
                        weight_tv_evolve_i = self.weight_dict['tv_evolve']
                    else:
                        weight_tv_evolve_i = self.weight_dict['evolve'] * self.weight_dict['tv']
                    
                    if weight_tv_evolve_i > 0:
                        tv_loss_i = self.tv_crit(py_pred[i])
                        scalar_stats.update({f'evolve_tv_loss_{i}': tv_loss_i})
                        loss += tv_loss_i * weight_tv_evolve_i

        # CV loss ì¡°ê±´ ìˆ˜ì •: cvë¡œ ì‹œì‘í•˜ëŠ” ëª¨ë“  í‚¤ë¥¼ í™•ì¸
        cv_weight_keys = [k for k in self.weight_dict.keys() if k.startswith('cv')]
        if cv_weight_keys:
            if num_polys == 0:
                init_cv_loss = dummy
                coarse_cv_loss = dummy
            else:
                init_cv_loss = self.cv_crit(poly_init)
                coarse_cv_loss = self.cv_crit(poly_coarse)
                
            # Weight ê³„ì‚°: ê°œë³„ weight ìš°ì„ , ì—†ìœ¼ë©´ stage * cv weight ì‚¬ìš©
            if 'cv_init' in self.weight_dict:
                weight_cv_init = self.weight_dict['cv_init']
            else:
                weight_cv_init = self.weight_dict.get('init', 0) * self.weight_dict.get('cv', 0)
            if 'cv_coarse' in self.weight_dict:
                weight_cv_coarse = self.weight_dict['cv_coarse']
            else:
                weight_cv_coarse = self.weight_dict.get('coarse', 0) * self.weight_dict.get('cv', 0)
            
            if weight_cv_init > 0:
                scalar_stats.update({'init_cv_loss': init_cv_loss})
                loss += init_cv_loss * weight_cv_init
            if weight_cv_coarse > 0:
                scalar_stats.update({'coarse_cv_loss': coarse_cv_loss})
                loss += coarse_cv_loss * weight_cv_coarse
            
            # evolve CV loss - iterationë³„ ê°œë³„ weight ì§€ì› (TV lossì™€ ë™ì¼í•˜ê²Œ)
            if len(py_pred) > 0:
                for i in range(len(py_pred)):
                    # iterationë³„ ê°œë³„ weight í™•ì¸
                    if f'cv_evolve_{i}' in self.weight_dict:
                        weight_cv_evolve_i = self.weight_dict[f'cv_evolve_{i}']
                    elif 'cv_evolve' in self.weight_dict:
                        weight_cv_evolve_i = self.weight_dict['cv_evolve']
                    else:
                        weight_cv_evolve_i = self.weight_dict.get('evolve', 0) * self.weight_dict.get('cv', 0)
                    
                    if weight_cv_evolve_i > 0:
                        cv_loss_i = self.cv_crit(py_pred[i])
                        scalar_stats.update({f'evolve_cv_loss_{i}': cv_loss_i})
                        loss += cv_loss_i * weight_cv_evolve_i

        ## edge standard deviation loss (Edge Equal loss = eeq loss)
        if self.eeq_crit is not None:
            if num_polys == 0:
                init_eeq_loss = dummy
                coarse_eeq_loss = dummy
                evolve_eeq_loss = dummy
            else:
                init_eeq_loss = self.eeq_crit(poly_init)
                coarse_eeq_loss = self.eeq_crit(poly_coarse)
                evolve_eeq_loss = dummy
                for i in range(len(py_pred)):
                    evolve_eeq_loss += self.eeq_crit(py_pred[i]) / len(py_pred)

            if 'edge_std_init' in self.weight_dict:
                weight_eeq_init = self.weight_dict['edge_std_init']
            else:
                weight_eeq_init = self.weight_dict['init'] * self.weight_dict['edge_std']
            if 'edge_std_coarse' in self.weight_dict:
                weight_eeq_coarse = self.weight_dict['edge_std_coarse']
            else:
                weight_eeq_coarse = self.weight_dict['coarse'] * self.weight_dict['edge_std']
            if 'edge_std_evolve' in self.weight_dict:
                weight_eeq_evolve = self.weight_dict['edge_std_evolve']
            else:
                weight_eeq_evolve = self.weight_dict['evolve'] * self.weight_dict['edge_std']

            if weight_eeq_init > 0:
                scalar_stats.update({'init_eeq_loss': init_eeq_loss})
                loss += init_eeq_loss * weight_eeq_init
            if weight_eeq_coarse > 0:
                scalar_stats.update({'coarse_eeq_loss': coarse_eeq_loss})
                loss += coarse_eeq_loss * weight_eeq_coarse
            if weight_eeq_evolve > 0:
                scalar_stats.update({'evolve_eeq_loss': evolve_eeq_loss})
                loss += evolve_eeq_loss * weight_eeq_evolve

        ## region
        if self.cfg.model.with_rasterize_net:
            for py_name in ('init', 'coarse'):
                gt_masks = self._create_targets(
                    output[f'img_gt_{py_name}_polys'].clone().detach().cpu().numpy(),
                    [int(self.cfg.data.input_w / self.cfg.data.down_ratio),
                     int(self.cfg.data.input_h / self.cfg.data.down_ratio)])
                with torch.no_grad():
                    gt_masks = torch.from_numpy(gt_masks).to(output['pred_mask'][py_name].device).long()
                    gt_masks.requires_grad = False
                region_loss = self.region_crit(output['pred_mask'][py_name], gt_masks)
                if 'region' in self.weight_dict:
                    weight_region_crit = self.weight_dict['region'] * self.weight_dict[py_name]
                else:
                    weight_region_crit = self.weight_dict[py_name]

                scalar_stats.update({f'region_{py_name}_loss': region_loss})
                loss += region_loss * weight_region_crit

            for pyi in range(len(py_pred)):
                py_name = f'py{pyi}'
                gt_masks = self._create_targets(py_pred[pyi].clone().detach().cpu().numpy(),
                    [int(self.cfg.data.input_w / self.cfg.data.down_ratio),
                     int(self.cfg.data.input_h / self.cfg.data.down_ratio)])
                with torch.no_grad():
                    gt_masks = torch.from_numpy(gt_masks).to(output['pred_mask'][py_name].device).long()
                    gt_masks.requires_grad = False
                region_loss = self.region_crit(output['pred_mask'][py_name], gt_masks)
                if 'region' in self.weight_dict:
                    weight_region_crit = self.weight_dict['region'] * self.weight_dict['evolve']
                else:
                    weight_region_crit = self.weight_dict['evolve']
                scalar_stats.update({f'region_{py_name}_loss': region_loss})
                loss += region_loss * weight_region_crit

        if self.cfg.model.with_sharp_contour:
            if epoch >= self.cfg.train.sharp_param['ipc_start_epoch']:
                if 'ipc' in output:
                    n_iter_ipc = len(output['ipc'])
                    for i_ipc in range(n_iter_ipc):
                        ipc_loss = self.ipc_crit(output['ipc'][i_ipc], output['ipc_gt'][i_ipc])
                        scalar_stats.update({f'ipc_loss_{i_ipc}': ipc_loss})
                        loss += ipc_loss * self.weight_dict['ipc']

                if 'ipc_random' in output:
                    n_iter_ipc = len(output['ipc_random'])
                    ipc_loss_random = dummy
                    for i_ipc in range(n_iter_ipc):
                        ipc_loss_random += self.ipc_crit(output['ipc_random'][i_ipc],
                                                         output['ipc_gt_random'][i_ipc])
                    if self.cfg.train.sharp_param['avg_ipc_random_loss']:
                        ipc_loss_random /= n_iter_ipc
                    scalar_stats.update({f'ipc_random_loss(M={n_iter_ipc})': ipc_loss_random})
                    loss += ipc_loss_random * self.weight_dict['ipc']

            if self.cfg.train.sharp_param['train_with_refine'] and epoch >= self.cfg.train.sharp_param['refine_start_epoch']:
                if self.cfg.train.sharp_param['refine_with_dml'] and epoch >= self.start_epoch:
                    num_sharp_iter = len(output['py_pred']) - self.cfg.model.evolve_iters
                    refine_dm_loss = dummy
                    for i_sharp in range(self.cfg.model.evolve_iters, len(output['py_pred'])):
                        if num_polys == 0:
                            part_refine_dm_loss = dummy
                            len_refine_loss = dummy
                        else:
                            # ğŸš¨ DDP-safe check: Ground Truthê°€ ì—†ëŠ” ê²½ìš°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
                            if len(output['img_gt_polys']) == 0:
                                part_refine_dm_loss = dummy
                            else:
                                part_refine_dm_loss = self.dml_crit(output['py_pred'][i_sharp - 1],
                                                                    output['py_pred'][i_sharp],
                                                                    output['img_gt_polys'],
                                                                    keyPointsMask)
                            len_refine_loss = torch.mean(torch.mean(torch.norm(output['py_pred'][i_sharp] - torch.roll(output['py_pred'][i_sharp], 1, -2), dim=-1), -1))
                        refine_dm_loss += (part_refine_dm_loss + len_refine_loss * self.weight_dict[
                            'length']) / num_sharp_iter
                        scalar_stats.update(
                            {'sharp_loss_{}'.format(i_sharp - self.cfg.model.evolve_iters): part_refine_dm_loss})
                        scalar_stats.update({f'len_sharp{i_sharp - self.cfg.model.evolve_iters}_loss': len_refine_loss})
                    loss += refine_dm_loss * self.weight_dict['sharp']
                else:
                    num_sharp_iter = len(output['py_pred']) - self.cfg.model.evolve_iters
                    refine_py_loss = dummy
                    for i_sharp in range(self.cfg.model.evolve_iters, len(output['py_pred'])):
                        if num_polys == 0:
                            part_py_loss = dummy
                            len_refine_loss = dummy
                        else:
                            part_py_loss = self.py_crit(output['py_pred'][i_sharp], output['img_gt_polys'])
                            len_refine_loss = torch.mean(
                                torch.mean(torch.norm(output['py_pred'][i_sharp] - torch.roll(
                                    output['py_pred'][i_sharp], 1, -2), dim=-1), -1))
                        refine_py_loss += (part_py_loss + len_refine_loss * self.weight_dict[
                            'length']) / num_sharp_iter
                        scalar_stats.update(
                            {'sharp_loss_{}'.format(i_sharp - self.cfg.model.evolve_iters): part_py_loss})
                        scalar_stats.update({f'len_sharp{i_sharp - self.cfg.model.evolve_iters}_loss': len_refine_loss})

                    loss += refine_py_loss * self.weight_dict['sharp']

        ## knowledge distillation
        if output_t is not None:
            # feature
            part = 'base'
            part_list = []
            for key_loss in self.cfg.train.kd_param['losses']:
                if f'ft_{part}' in key_loss:
                    part_list.append(key_loss)
            for each_kd_loss in part_list:
                pos_feature = int(each_kd_loss.split('_')[-1])
                kd_inter_loss = self.kd_inter_crit(output['feature_banks'][part][pos_feature], output_t['feature_banks'][part][pos_feature])
                scalar_stats.update({f'kd_{each_kd_loss}_loss': kd_inter_loss})
                if f'kd_{each_kd_loss}' in self.weight_dict:
                    weight_kd = self.weight_dict[f'kd_{each_kd_loss}']
                elif f'kd_ft_{part}' in self.weight_dict:
                    weight_kd = self.weight_dict[f'kd_ft_{part}']
                elif 'kd' in self.weight_dict:
                    weight_kd = self.weight_dict['kd']
                else:
                    weight_kd = 1.
                loss += weight_kd * kd_inter_loss

            for part in ['cnn_feature','feature_coarse']:
                if part in self.cfg.train.kd_param['losses']:
                    kd_inter_loss = self.kd_inter_crit(output[part], output_t[part])
                    scalar_stats.update({f'kd_{part}_loss': kd_inter_loss})
                    if f'kd_{part}' in self.weight_dict:
                        weight_kd = self.weight_dict[f'kd_{part}']
                    elif 'kd' in self.weight_dict:
                        weight_kd = self.weight_dict['kd']
                    else:
                        weight_kd = 1.
                    loss += weight_kd * kd_inter_loss
            # pixel
            part = 'pixel'
            if part in self.cfg.train.kd_param['losses']:
                target = pixel_gt.bool().long()
                kd_ct_loss = getattr(self, f"kd_{self.cfg.train.kd_param['losses'][part]}_crit")(
                    output[f'{part}'], output_t[f'{part}'], target)
                scalar_stats.update({f'kd_{part}_loss': kd_ct_loss})
                if f'kd_{part}' in self.weight_dict:
                    weight_kd = self.weight_dict[f'kd_{part}']
                elif 'kd' in self.weight_dict:
                    weight_kd = self.weight_dict['kd']
                else:
                    weight_kd = 1.
                if self.cfg.train.kd_param['weight_type'] == 'normalized':
                    if weight_kd >= 1.:
                        weight_kd /= (1 + weight_kd)
                    weight_kd *= self.weight_dict[f'{part}']
                loss += weight_kd * kd_ct_loss
            # center
            part = 'ct'
            if part in self.cfg.train.kd_param['losses']:
                kd_ct_loss = getattr(self, f"kd_{self.cfg.train.kd_param['losses'][part]}_crit")(output[f'{part}_hm'], output_t[f'{part}_hm'], batch[f'{part}_hm'])
                scalar_stats.update({f'kd_{part}_loss': kd_ct_loss})
                if f'kd_{part}' in self.weight_dict:
                    weight_kd = self.weight_dict[f'kd_{part}']
                elif 'kd' in self.weight_dict:
                    weight_kd = self.weight_dict['kd']
                else:
                    weight_kd = 1.
                if self.cfg.train.kd_param['weight_type'] == 'normalized':
                    if weight_kd >= 1.:
                        weight_kd /= (1 + weight_kd)
                    weight_kd *= self.weight_dict[f'box_{part}']
                loss += weight_kd * kd_ct_loss

            # init & coarse
            for part in ['init','coarse']:
                if part in self.cfg.train.kd_param['losses']:
                    kd_py_loss = getattr(self, f"kd_{self.cfg.train.kd_param['losses'][part]}_crit")(
                        output[f'poly_{part}'], output_t[f'poly_{part}'], output['img_gt_polys'])
                    scalar_stats.update({f'kd_py_loss_{part}': kd_py_loss})
                    if f'kd_{part}' in self.weight_dict:
                        weight_kd = self.weight_dict[f'kd_{part}']
                    elif 'kd' in self.weight_dict:
                        weight_kd = self.weight_dict['kd']
                    else:
                        weight_kd = 1.
                    if self.cfg.train.kd_param['weight_type']=='normalized':
                        if weight_kd >= 1.:
                            weight_kd /= (1 + weight_kd)
                        weight_kd *= self.weight_dict[part]
                    loss += weight_kd * kd_py_loss

            # evolve (snake)
            for i in range(self.cfg.model.evolve_iters):
                if f'evolve_{i}' in self.cfg.train.kd_param['losses']:
                    kd_py_loss = getattr(self,f"kd_{self.cfg.train.kd_param['losses'][f'evolve_{i}']}_crit")(output['py_pred'][i], output_t['py_pred'][i], output['img_gt_polys'])
                    scalar_stats.update({f'kd_py_loss_{i}': kd_py_loss})
                    if f'kd_py_{i}' in self.weight_dict:
                        weight_kd = self.weight_dict[f'kd_py_{i}']
                    elif 'kd' in self.weight_dict:
                        weight_kd = self.weight_dict['kd']
                    else:
                        weight_kd = 1.
                    if self.cfg.train.kd_param['weight_type']=='normalized':
                        if weight_kd >= 1.:
                            weight_kd /= (1 + weight_kd)
                        weight_kd *= self.weight_dict['evolve']
                    loss += weight_kd * kd_py_loss

        scalar_stats.update({'loss': loss})
        if not torch.isfinite(loss): #edit:debug:ddp-stop:25-08-09
            # ê·¸ë˜í”„ ìœ ì§€ + 0ìœ¼ë¡œ í´ë¨í”„
            loss = (loss * 0.0) + dummy
            # ì›ì¸ ì°¾ìœ¼ë ¤ë©´ ë¡œê·¸ ì¶”ê°€
            print(f"[R{dist.get_rank()}] non-finite loss at epoch={batch['epoch']} step?", flush=True)

        return output, loss, scalar_stats, out_ontraining

    @torch.no_grad()
    def _create_targets(self, instances, img_hw, lid=0):
        masks = []
        for obj_i in range(instances.shape[0]):
            instance = instances[obj_i].astype(np.int32)
            mask = np.zeros((img_hw[0], img_hw[1], 1), np.uint8)
            masks.append(cv2.fillPoly(mask, [instance], 1))
        masks = np.stack(masks, axis=0)  # (Nc, H, W, 1)
        return masks.squeeze(-1)
