import torch.nn as nn
import torch
import torch.distributed as dist

def _assert_same_across_ranks(flag: bool, what: str, device): #edit:ddp-sync-bn-dummy:25-08-09
    if not dist.is_initialized():
        return
    t = torch.tensor([int(flag)], device=device)
    t_max = t.clone(); t_min = t.clone()
    dist.all_reduce(t_max, op=dist.ReduceOp.MAX)
    dist.all_reduce(t_min, op=dist.ReduceOp.MIN)
    if t_max.item() != t_min.item():
        rank = dist.get_rank()
        raise RuntimeError(f"[Rank {rank}] {what} mismatch across ranks")

class CircConv(nn.Module):
    def __init__(self, state_dim, out_state_dim=None, n_adj=4, with_img_idx=False, refine_kernel_size=3):
        super(CircConv, self).__init__()
        self.with_img_idx = with_img_idx
        self.refine_kernel_size = refine_kernel_size
        self.n_adj = n_adj
        out_state_dim = state_dim if out_state_dim is None else out_state_dim
        if self.with_img_idx:
            self.fc = nn.Conv2d(state_dim, out_state_dim, kernel_size=(self.n_adj * 2 + 1, self.refine_kernel_size), dilation=(self.dilation, 1),
                                padding='same', padding_mode='circular')
        else:
            self.fc = nn.Conv1d(state_dim, out_state_dim, kernel_size=self.n_adj*2+1, dilation=self.dilation, padding='same', padding_mode='circular')

    def forward(self, input):
        # if self.n_adj != 0:
        #     input = torch.cat([input[..., -self.n_adj:], input, input[..., :self.n_adj]], dim=2)
        return self.fc(input)

class DilatedCircConv(nn.Module):
    def __init__(self, state_dim, out_state_dim=None, n_adj=4, dilation=1, with_img_idx=False, refine_kernel_size=3):
        super(DilatedCircConv, self).__init__()
        self.with_img_idx = with_img_idx
        self.refine_kernel_size = refine_kernel_size
        self.n_adj = n_adj
        self.dilation = dilation
        out_state_dim = state_dim if out_state_dim is None else out_state_dim
        if self.with_img_idx:
            self.fc = nn.Conv2d(state_dim, out_state_dim, kernel_size=(self.n_adj*2+1, self.refine_kernel_size), padding='same', padding_mode='circular', dilation=(self.dilation, 1))
        else:
            self.fc = nn.Conv1d(state_dim, out_state_dim, kernel_size=self.n_adj*2+1, dilation=self.dilation)

    def forward(self, input):
        if self.n_adj != 0 and (not self.with_img_idx):
            input = torch.cat([input[..., -self.n_adj*self.dilation:], input, input[..., :self.n_adj*self.dilation]], dim=2)
        if input.size(-1) == 0:
            return self.fc(input.transpose(-1, 0))
        else:
            return self.fc(input)


# CircConv2D class removed - only using flatten mode now


_conv_factory = {
    'grid': CircConv,
    'dgrid': DilatedCircConv
}

class BasicBlock(nn.Module):
    def __init__(self, state_dim, out_state_dim, conv_type, n_adj=4, dilation=1, with_img_idx=False, refine_kernel_size=3):
        super(BasicBlock, self).__init__()
        self.with_img_idx = with_img_idx
        if conv_type == 'grid':
            self.conv = _conv_factory[conv_type](state_dim, out_state_dim, n_adj, with_img_idx=with_img_idx, refine_kernel_size=refine_kernel_size)
        else:
            self.conv = _conv_factory[conv_type](state_dim, out_state_dim, n_adj, dilation, with_img_idx=with_img_idx, refine_kernel_size=refine_kernel_size)
        self.relu = nn.ReLU(inplace=True)
        if self.with_img_idx:
            self.norm = nn.BatchNorm2d(out_state_dim)
        else:
            self.norm = nn.BatchNorm1d(out_state_dim)

    @torch.no_grad()
    def _run_dummy_bn(self, ref_tensor: torch.Tensor):
        """ë¹ˆ ë°°ì¹˜ì¼ ë•Œ ëª¨ë“  rankì—ì„œ BN collectiveë¥¼ 1íšŒ ë§ì¶°ì£¼ê¸° ìœ„í•œ ë”ë¯¸ í˜¸ì¶œ (edit:ddp-sync-bn-dummy:25-08-09)"""
        C = self.norm.num_features
        # ref_tensorì˜ ì°¨ì› ìˆ˜ì— ë§ì¶° ì±„ë„ ì¶•ë§Œ ë§ëŠ” ìµœì†Œ ë”ë¯¸ë¥¼ ë§Œë“¦
        dims = ref_tensor.dim()
        if self.with_img_idx:
            # BN2d: [N, C, H, W]
            dummy = torch.zeros(1, C, 1, 1, device=ref_tensor.device, dtype=ref_tensor.dtype)
        else:
            # BN1d: [N, C] ë˜ëŠ” [N, C, L] ëª¨ë‘ OK â†’ [1, C, 1]ë¡œ ì•ˆì „í•˜ê²Œ
            shape = (1, C, 1) if dims >= 3 else (1, C)
            dummy = torch.zeros(*shape, device=ref_tensor.device, dtype=ref_tensor.dtype)
        # training ëª¨ë“œì—ì„œ ì‹¤í–‰ë˜ì–´ì•¼ collectiveê°€ ë™ì‘í•¨ (evalì€ collective ì—†ìŒ)
        was_training = self.norm.training
        self.norm.train(True)
        _ = self.norm(dummy)
        self.norm.train(was_training)

    def forward(self, x):
        # # 1) ì…ë ¥ ìì²´ê°€ ë¹„ì—ˆìœ¼ë©´ conv/reluë„ ìƒëµí•˜ê³  ë”ë¯¸ BNë§Œ ë§ì¶˜ ë’¤ ê·¸ëŒ€ë¡œ ë°˜í™˜ (edit:ddp-sync-bn-dummy:25-08-09)
        # if isinstance(x, torch.Tensor) and (x.numel() == 0 or x.shape[0] == 0):
        #     self._run_dummy_bn(x)
        #     return x  # ë¹ˆ í…ì„œ ê·¸ëŒ€ë¡œ
        #
        # x = self.conv(x)
        # x = self.relu(x)
        # # 2) conv ê²°ê³¼ê°€ ë¹„ì–´ë„ ë™ì¼ ì²˜ë¦¬ (ìƒë¥˜ ëª¨ë“ˆì—ì„œ í•„í„°ë§ë˜ë©´ ì—¬ê¸°ì„œ ë¹„ì–´ì§ˆ ìˆ˜ ìˆìŒ) (edit:ddp-sync-bn-dummy:25-08-09)
        # if isinstance(x, torch.Tensor) and (x.numel() == 0 or x.shape[0] == 0):
        #     self._run_dummy_bn(x)
        #     return x
        #
        # # 3) ì •ìƒ ê²½ë¡œ
        # x = self.norm(x)
        # ---- ì—¬ê¸°ê¹Œì§€ëŠ” ì›ë˜ ê²½ë¡œ ----
        x = self.conv(x)
        x = self.relu(x)

        # ====== SyncBN ì •í•© ê°•ì œ êµ¬ê°„ (edit:ddp-sync-bn-dummy:25-08-09) ======
        is_syncbn = isinstance(self.norm, nn.SyncBatchNorm)

        # 1) BNì˜ training/eval ëª¨ë“œê°€ ì „ rankì—ì„œ ê°™ë„ë¡ ë³´ì¥
        #    (ì¼ë¶€ ë¶„ê¸°ì—ì„œ .eval() ëœ ìƒíƒœê°€ ì„ì´ë©´ collective ë¶ˆì¼ì¹˜ ë°œìƒ)
        # _assert_same_across_ranks(self.norm.training, "SyncBatchNorm.training", x.device)

        # 2) AMP/dtype ì •í•©: BNì€ FP32ë¡œ ê³ ì •í•´ í˜¸ì¶œ (rankë³„ autocast on/off ë¶ˆì¼ì¹˜ ë°©ì§€)
        if is_syncbn:
            with torch.cuda.amp.autocast(enabled=False):
                x = self.norm(x.float())
        else:
            x = self.norm(x)
        # ==================================
        return x


class BasicBlock2D(nn.Module):
    """BasicBlock for 2D inputs (first layer only)"""
    def __init__(self, state_dim, out_state_dim, n_adj=4, spatial_kernel=3):
        super(BasicBlock2D, self).__init__()
        # CircConv2D expects the input channel count, not the base feature dimension
        # It will internally flatten spatial dimensions from [C, H, W] to [C*H*W]
        self.conv = CircConv2D(state_dim, out_state_dim, n_adj=n_adj, spatial_kernel=spatial_kernel)
        self.relu = nn.ReLU(inplace=True)
        self.norm = nn.BatchNorm1d(out_state_dim)  # 1D BatchNorm for output [N, C, V]

    @torch.no_grad()
    def _run_dummy_bn(self, ref_tensor: torch.Tensor):
        """ë¹ˆ ë°°ì¹˜ì¼ ë•Œ ëª¨ë“  rankì—ì„œ BN collectiveë¥¼ 1íšŒ ë§ì¶°ì£¼ê¸° ìœ„í•œ ë”ë¯¸ í˜¸ì¶œ"""
        C = self.norm.num_features
        # BN1d: [1, C, 1] í˜•íƒœë¡œ ë”ë¯¸ ìƒì„±
        dummy = torch.zeros(1, C, 1, device=ref_tensor.device, dtype=ref_tensor.dtype)
        was_training = self.norm.training
        self.norm.train(True)
        _ = self.norm(dummy)
        self.norm.train(was_training)

    def forward(self, x):
        """
        x: [N_poly, C, N_vert, 3, 3] for 2D input
        output: [N_poly, out_dim, N_vert]
        """
        x = self.conv(x)  # [N_poly, out_dim, N_vert]
        x = self.relu(x)

        # SyncBN ì •í•© ê°•ì œ êµ¬ê°„ (ê¸°ì¡´ê³¼ ë™ì¼)
        is_syncbn = isinstance(self.norm, nn.SyncBatchNorm)
        
        if is_syncbn:
            with torch.cuda.amp.autocast(enabled=False):
                x = self.norm(x.float())
        else:
            x = self.norm(x)
            
        return x


# class Snake(nn.Module):
#     def __init__(self, state_dim, feature_dim, conv_type='dgrid', with_img_idx=False, refine_kernel_size=3):
#         super(Snake, self).__init__()
#         self.with_img_idx = with_img_idx
#         self.head = BasicBlock(feature_dim, state_dim, conv_type, with_img_idx=with_img_idx, refine_kernel_size=refine_kernel_size)
#         self.res_layer_num = 7
#         dilation = [1, 1, 1, 2, 2, 4, 4]
#         n_adj = 4
#         for i in range(self.res_layer_num):
#             if dilation[i] == 0:
#                 conv_type = 'grid'
#             else:
#                 conv_type = 'dgrid'
#             conv = BasicBlock(state_dim, state_dim, conv_type, n_adj=n_adj, dilation=dilation[i], with_img_idx=with_img_idx, refine_kernel_size=refine_kernel_size)
#             self.__setattr__('res'+str(i), conv)
#
#         fusion_state_dim = 256
#
#         if self.with_img_idx:
#             self.fusion = nn.Conv2d(state_dim * (self.res_layer_num + 1), fusion_state_dim, (1, refine_kernel_size), padding='same')
#             self.prediction = nn.Sequential(
#                 nn.Conv2d(state_dim * (self.res_layer_num + 1) + fusion_state_dim, 256, (1, refine_kernel_size), padding='same'),
#                 nn.ReLU(inplace=True),
#                 nn.Conv2d(256, 64, (1, refine_kernel_size), padding='same'),
#                 nn.ReLU(inplace=True),
#                 nn.Conv2d(64, 2, (1, refine_kernel_size), padding='same')
#             )
#         else:
#             self.fusion = nn.Conv1d(state_dim * (self.res_layer_num + 1), fusion_state_dim, refine_kernel_size, padding=(refine_kernel_size-1)//2)
#             self.prediction = nn.Sequential(
#                 nn.Conv1d(state_dim * (self.res_layer_num + 1) + fusion_state_dim, 256, refine_kernel_size, padding=(refine_kernel_size-1)//2),
#                 nn.ReLU(inplace=True),
#                 nn.Conv1d(256, 64, refine_kernel_size, padding=(refine_kernel_size-1)//2),
#                 nn.ReLU(inplace=True),
#                 nn.Conv1d(64, 2, refine_kernel_size, padding=(refine_kernel_size-1)//2)
#             )
#
#     def forward(self, x):
#         states = []
#         x = self.head(x)
#         states.append(x)
#         for i in range(self.res_layer_num):
#             x = self.__getattr__('res'+str(i))(x) + x
#             states.append(x)
#
#         state = torch.cat(states, dim=1)
#
#         global_state = torch.max(self.fusion(state), dim=2, keepdim=True)[0]
#         if self.with_img_idx:
#             global_state = global_state.expand(global_state.size(0), global_state.size(1), state.size(2), global_state.size(3))
#         else:
#             global_state = global_state.expand(global_state.size(0), global_state.size(1), state.size(2))
#         state = torch.cat([global_state, state], dim=1)
#
#         # print(f"[DEBUG] state.shape = {state.shape}, dtype={state.dtype}, device={state.device}")
#         x = self.prediction(state)
#
#         return x
class Snake(nn.Module):
    def __init__(self, state_dim, feature_dim, conv_type='dgrid', with_img_idx=False, refine_kernel_size=3,
                 use_vertex_classifier=False, common_prediction_type=1, vtx_cls_kernel_size=1,
                 use_3x3_feature=False, feature_3x3_mode='flatten'):
        super(Snake, self).__init__()
        self.with_img_idx = with_img_idx
        self.use_vertex_classifier = use_vertex_classifier  # ğŸ‘ˆ ì˜µì…˜ ì¶”ê°€
        self.common_prediction_type = common_prediction_type
        self.use_3x3_feature = use_3x3_feature  # 3x3 feature ì‚¬ìš© ì—¬ë¶€
        self.feature_3x3_mode = feature_3x3_mode  # 'flatten' or 'conv2d'
        
        # Create head - feature_dim already includes flattening if needed
        # (get_gcn_feature_3x3 already flattens C*9 channels before passing here)
        head_feature_dim = feature_dim
        self.head = BasicBlock(head_feature_dim, state_dim, conv_type, with_img_idx=with_img_idx,
                               refine_kernel_size=refine_kernel_size)
        self.res_layer_num = 7
        dilation = [1, 1, 1, 2, 2, 4, 4]
        n_adj = 4
        for i in range(self.res_layer_num):
            conv_type_i = 'grid' if dilation[i] == 0 else 'dgrid'
            conv = BasicBlock(state_dim, state_dim, conv_type_i, n_adj=n_adj, dilation=dilation[i],
                              with_img_idx=with_img_idx, refine_kernel_size=refine_kernel_size)
            self.__setattr__('res' + str(i), conv)

        fusion_state_dim = 256

        if self.with_img_idx:
            self.fusion = nn.Conv2d(state_dim * (self.res_layer_num + 1), fusion_state_dim, (1, refine_kernel_size),
                                    padding='same')
            self.prediction = nn.Sequential(
                nn.Conv2d(state_dim * (self.res_layer_num + 1) + fusion_state_dim, 256, (1, refine_kernel_size),
                          padding='same'),
                nn.ReLU(inplace=True),
                nn.Conv2d(256, 64, (1, refine_kernel_size), padding='same'),
                nn.ReLU(inplace=True),
                nn.Conv2d(64, 2, (1, refine_kernel_size), padding='same')
            )
        else:
            self.fusion = nn.Conv1d(state_dim * (self.res_layer_num + 1), fusion_state_dim, refine_kernel_size,
                                    padding=(refine_kernel_size - 1) // 2)

            # Calculate input channels for prediction layers
            prediction_input_dim = state_dim * (self.res_layer_num + 1) + fusion_state_dim
            
            if self.use_vertex_classifier:
                if self.common_prediction_type == 1:
                    self.common_prediction = nn.Sequential(
                        nn.Conv1d(prediction_input_dim, 256, refine_kernel_size,
                                  padding=(refine_kernel_size - 1) // 2),
                        nn.ReLU(inplace=True)
                    )
                    self.offset_prediction = nn.Sequential(
                        nn.Conv1d(256, 64, refine_kernel_size, padding=(refine_kernel_size - 1) // 2),
                        nn.ReLU(inplace=True),
                        nn.Conv1d(64, 2, refine_kernel_size, padding=(refine_kernel_size - 1) // 2)
                    )
                    self.vertex_classifier = nn.Sequential(
                        nn.Conv1d(256, 64, kernel_size=vtx_cls_kernel_size, padding='same'),
                        nn.ReLU(inplace=True),
                        nn.Conv1d(64, 2, kernel_size=vtx_cls_kernel_size, padding='same')
                    )

                elif self.common_prediction_type == 2:
                    self.common_prediction = nn.Sequential(
                        nn.Conv1d(prediction_input_dim, 256, refine_kernel_size,
                                  padding=(refine_kernel_size - 1) // 2),
                        nn.ReLU(inplace=True),
                        nn.Conv1d(256, 64, refine_kernel_size, padding=(refine_kernel_size - 1) // 2),
                        nn.ReLU(inplace=True)
                    )
                    self.offset_prediction = nn.Sequential(
                        nn.Conv1d(64, 2, refine_kernel_size, padding=(refine_kernel_size - 1) // 2)
                    )
                    self.vertex_classifier = nn.Sequential(
                        nn.Conv1d(64, 2, kernel_size=vtx_cls_kernel_size, padding='same')
                    )

                elif self.common_prediction_type == 0:
                    self.offset_prediction = nn.Sequential(
                        nn.Conv1d(prediction_input_dim, 256, refine_kernel_size,
                                  padding=(refine_kernel_size - 1) // 2),
                        nn.ReLU(inplace=True),
                        nn.Conv1d(256, 64, refine_kernel_size, padding=(refine_kernel_size - 1) // 2),
                        nn.ReLU(inplace=True),
                        nn.Conv1d(64, 2, refine_kernel_size, padding=(refine_kernel_size - 1) // 2)
                    )
                    self.vertex_classifier = nn.Sequential(
                        nn.Conv1d(prediction_input_dim, 256, kernel_size=vtx_cls_kernel_size, padding='same'),
                        nn.ReLU(inplace=True),
                        nn.Conv1d(256, 64, kernel_size=vtx_cls_kernel_size, padding='same'),
                        nn.ReLU(inplace=True),
                        nn.Conv1d(64, 2, kernel_size=vtx_cls_kernel_size, padding='same')
                    )
            else:
                self.prediction = nn.Sequential(
                    nn.Conv1d(prediction_input_dim, 256, refine_kernel_size,
                              padding=(refine_kernel_size - 1) // 2),
                    nn.ReLU(inplace=True),
                    nn.Conv1d(256, 64, refine_kernel_size, padding=(refine_kernel_size - 1) // 2),
                    nn.ReLU(inplace=True),
                    nn.Conv1d(64, 2, refine_kernel_size, padding=(refine_kernel_size - 1) // 2)
                )

    def forward(self, x, return_vertex_classifier=False):
        states = []
        coord_part = None  # Initialize coordinate part
        
        # Standard processing for all modes
        x = self.head(x)
        
        # Process through residual layers with clean state_dim channels
        states.append(x)
        for i in range(self.res_layer_num):
            x = self.__getattr__('res' + str(i))(x) + x
            states.append(x)

        state = torch.cat(states, dim=1)

        global_state = torch.max(self.fusion(state), dim=2, keepdim=True)[0]
        if self.with_img_idx:
            global_state = global_state.expand(global_state.size(0), global_state.size(1), state.size(2),
                                               global_state.size(3))
        else:
            global_state = global_state.expand(global_state.size(0), global_state.size(1), state.size(2))
        state = torch.cat([global_state, state], dim=1)
        
        # No coordinate concatenation needed in simplified version

        if self.use_vertex_classifier:
            if self.common_prediction_type == 0:
                offsets = self.offset_prediction(state)
                logits = self.vertex_classifier(state)
            else:
                shared = self.common_prediction(state)
                offsets = self.offset_prediction(shared)
                logits = self.vertex_classifier(shared)

            if return_vertex_classifier:
                return offsets, logits  # x: offset prediction, logits: (B, 1, N)
            else:
                return offsets
        else:
            x = self.prediction(state)  # shape: (B, 2, N)
            return x

    #edit:ddp-sync-bn-dummy:25-08-09
    @torch.no_grad()
    def run_dummy_like(self, ref: torch.Tensor):
        # í•­ìƒ FP32ë¡œ BN í˜¸ì¶œ (AMP ë¶ˆì¼ì¹˜ ë°©ì§€)
        with torch.cuda.amp.autocast(enabled=False):
            device = ref.device
            for m in self.modules():
                if isinstance(m, nn.SyncBatchNorm):
                    C = m.num_features
                    was_training = m.training
                    m.train(True)
                    ok = False
                    for shape in [(1, C, 1, 1), (1, C, 1), (1, C)]:
                        try:
                            dummy = torch.zeros(*shape, device=device, dtype=torch.float32)
                            m(dummy)
                            ok = True
                            break
                        except Exception:
                            pass
                    if not ok:
                        # ë§ˆì§€ë§‰ ì•ˆì „ì¥ì¹˜: ê·¸ë˜ë„ í•œ ë²ˆì€ í˜¸ì¶œë˜ê²Œ
                        dummy = torch.zeros(1, C, device=device, dtype=torch.float32)
                        m(dummy)
                    m.train(was_training)


class AggregateCirConv(nn.Module):
    def __init__(self, state_dim, feature_dim, conv_type='dgrid', with_img_idx=False, refine_kernel_size=3, type='default',
                 fusion_conv_num=3, fusion_state_dim=256, refine_kernel_size_fusion=1):
        super(AggregateCirConv, self).__init__()
        self.aggregate_type = type #(default, snake)
        self.fusion_conv_num = fusion_conv_num
        self.fusion_state_dim = fusion_state_dim
        self.with_img_idx = with_img_idx
        self.head = BasicBlock(feature_dim, state_dim, conv_type, with_img_idx=with_img_idx,
                               refine_kernel_size=refine_kernel_size)
        self.res_layer_num = 7
        dilation = [1, 1, 1, 2, 2, 4, 4]
        n_adj = 4
        for i in range(self.res_layer_num):
            if dilation[i] == 0:
                conv_type = 'grid'
            else:
                conv_type = 'dgrid'
            conv = BasicBlock(state_dim, state_dim, conv_type, n_adj=n_adj, dilation=dilation[i],
                              with_img_idx=with_img_idx, refine_kernel_size=refine_kernel_size)
            self.__setattr__('res' + str(i), conv)

        if self.aggregate_type == 'snake':
            self.fusion = nn.Conv1d(state_dim * (self.res_layer_num + 1), fusion_state_dim, refine_kernel_size,
                                    padding=(refine_kernel_size - 1) // 2)
            self.prediction = nn.Sequential(
                nn.Conv1d(state_dim * (self.res_layer_num + 1) + fusion_state_dim, 256, refine_kernel_size,
                          padding=(refine_kernel_size - 1) // 2),
                nn.ReLU(inplace=True),
                nn.Conv1d(256, 64, refine_kernel_size, padding=(refine_kernel_size - 1) // 2),
            )
        else:
            self.fusion = nn.Sequential()
            pre_dim = state_dim * (self.res_layer_num + 1)
            for fusion_i in range(self.fusion_conv_num):
                if fusion_i == (self.fusion_conv_num-1):
                    self.fusion.add_module(
                        f"conv1d_{fusion_i}", nn.Conv1d(pre_dim, 64, refine_kernel_size_fusion)
                    )
                    self.fusion.add_module(
                        f"relu_{fusion_i}", nn.ReLU(inplace=True)
                    )
                    continue
                self.fusion.add_module(
                    f"conv1d_{fusion_i}", nn.Conv1d(pre_dim, fusion_state_dim, refine_kernel_size_fusion)
                )
                self.fusion.add_module(
                    f"relu_{fusion_i}", nn.ReLU(inplace=True)
                )
                pre_dim = fusion_state_dim

    def forward(self, x):
        states = []
        x = self.head(x)
        states.append(x)
        for i in range(self.res_layer_num):
            x = self.__getattr__('res' + str(i))(x) + x
            states.append(x)

        state = torch.cat(states, dim=1)

        if self.aggregate_type == 'snake':
            global_state = torch.max(self.fusion(state), dim=2, keepdim=True)[0]
            global_state = global_state.expand(global_state.size(0), global_state.size(1), state.size(2))
            state = torch.cat([global_state, state], dim=1)
            x = self.prediction(state)
        else:
            x = self.fusion(state)
        return x
